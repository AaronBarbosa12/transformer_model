{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172f4e1c",
   "metadata": {},
   "source": [
    "# Implementing Transformers From Scratch With Tensorflow\n",
    "\n",
    "Transformers are machine learning models that have proven to be incredibly powerful for natural language processing (NLP) applications. Standard NLP tools such as LSTM or GRU networks suffer from vanishing gradients, therefore,  performance tends to suffer when processing long sequences. Transformers, however, use a special attention mechanism to bypass this vanishing gradient problem.\n",
    "\n",
    "In this project we create a portuguese-to-english translator in Tensorflow. We do this by training and building a transformer model from first principles. \n",
    "\n",
    "NOTE: \n",
    "- The model in this example is not optimized for performance. This project is for the purposes of learning about transformers in general.\n",
    "- This project was inspired by the following Tensorflow tutorial: https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "I'm using the same portuguese to english dataset as in the tutorial featured above ^. While implementing the transformer itself, however, I've avoided looking at any outside resources (aside from the original research paper **\"Attention Is All You Need\"**: https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba1578b",
   "metadata": {},
   "source": [
    "## Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1414170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/miniconda3/envs/kaggle/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Essential Imports\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Misc Imports\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dd64c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e17f4",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1682454a",
   "metadata": {},
   "source": [
    "We start by acquiring our dataset. Tensoflow datasets contains a collection of ~50k examples of portuguese text (as well as the corresponding english translation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ed5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 16:32:44.073517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', \n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357d23b",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cd820",
   "metadata": {},
   "source": [
    "Now that we have our dataset downloaded, we need to convert it into a vectorized format. There are many different tokenization techniques, but here we'll utilize the tokenizer that comes with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0f6d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a pre-fitted tokenizer \n",
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")\n",
    "tokenizers = tf.saved_model.load(f\"{model_name}\")\n",
    "\n",
    "pt_vocab_size = tokenizers.pt.get_vocab_size().numpy()\n",
    "en_vocab_size = tokenizers.en.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1bb4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the dataset\n",
    "def tokenize_data(_x,_y):\n",
    "    return tokenizers.pt.tokenize(_x).to_tensor(), tokenizers.en.tokenize(_y).to_tensor()\n",
    "\n",
    "def batch_data(_dataset, _batch_size):\n",
    "    return _dataset.batch(_batch_size).map(tokenize_data)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset_batched = batch_data(train_examples, batch_size)\n",
    "test_dataset_batched = batch_data(val_examples, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4f385",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641be5f3",
   "metadata": {},
   "source": [
    "Traditional recurrent neural networks function by processing tokens sequentially. A hidden state is calculated at each time step, and is then fed into the network at the next time step. Processing inputs sequentially can be slow, and as the the number of time steps increases, then network begins to \"forget\" about tokens at the start of the sequence. \n",
    "\n",
    "In contrast, transformer networks process the inputs in parallel. This is much faster, but there's one problem. Consider the following two sentences:\n",
    "\n",
    "- The cat ran over the lazy dog.\n",
    "- The dog ran over the lazy cat.\n",
    "\n",
    "The ordering of a sentence matters. These two sentences have very different meanings. But if we process inputs in parrallel, then these sentences are functionally the same. The network inputs contain no temporal information. We'll need to add it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        self.d_model = d_model\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "    \n",
    "    def pos_encoding_mask(self, inputs): \n",
    "        # pos - sentence index\n",
    "        # i - embedding index \n",
    "        # inputs - shape (batch_size, sentence_length, embedding_dim)\n",
    "\n",
    "        pos_arr = tf.range(start = 0, limit = tf.shape(inputs)[1] , dtype = tf.float32)[:, np.newaxis]\n",
    "        i_arr = tf.range(start = 0, limit = self.d_model , dtype = tf.float32)[np.newaxis, :]\n",
    "        \n",
    "        # assign a fixed frequency to each dimension of the embedding\n",
    "        omega = 1 / np.power(10000, (2 * (i_arr//2)) / np.float32(self.d_model))\n",
    "        angles = np.matmul(pos_arr, i_arr)\n",
    "        \n",
    "        pos_mask = np.zeros((tf.shape(inputs)[1], self.d_model))\n",
    "        pos_mask[:,0::2] = tf.math.cos(angles)[:,0::2]\n",
    "        pos_mask[:,1::2] = tf.math.sin(angles)[:,1::2]\n",
    "        pos_mask = tf.convert_to_tensor(pos_mask, dtype = tf.float32)\n",
    "        return pos_mask \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        pos_mask = self.pos_encoding_mask(inputs)\n",
    "        return tf.math.add(inputs, pos_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68677d",
   "metadata": {},
   "source": [
    "The solution proposed by the authors of \"Attention Is All You Need\":\n",
    "\n",
    "Suppose each token `token` at position `sentence_pos` is represented by a d-dimensional vector. We want to create a new representation `token_new = pos_mask + token` that encodes both the semantic information of `token`, while also containing information about its position in the sentence. \n",
    "\n",
    "One solution: assign a unique (fixed) frequency `f_embedding_dim` to each dimenstion of the word embedding. The value of `pos_mask` is determined by a sinusoid that oscillates with `f_embedding_dim` over `sentence_pos`:\n",
    "\n",
    "$f($ `sentence_pos` $)$ = $sin($ `f_embedding_dim` *`sentence_pos`$)$\n",
    "\n",
    "where `f_embedding_dim` is a number that increases with the value of `embedding_dim`. The idea is that as we reach higher values of `embedding_dim`, it becomes easier to resolve words that are close together, but more difficult to resolve words that are further apart. The opposite logic also applies to lower dimensions.\n",
    "\n",
    "The embedding used by the authors of \"Attention is All You Need\":\n",
    "![title](imgs/pos_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12a9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 20 # dimensionality of the word embedding\n",
    "n_heads = 8 # number of attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ee644",
   "metadata": {},
   "source": [
    "## Create Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94a5e4",
   "metadata": {},
   "source": [
    "The attention mechanism is a powerful technique used, in this case, to process sequences of text. Suppose we have an `n x d` array, representing an embedded sequence of tokens. After applying the attention mechanism, we want to end up with another `n x d` array, representing the original sentence after the attention block has filtered out and processed pertinent information. We call this `MultiHeadedAttention` because we'll stack multiple attention blocks together. Hopefully, each block attends to different parts of the sentence and learns to gather useful information.\n",
    "![title](imgs/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5322229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_heads, d_model ):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):        \n",
    "        # Create Self-Attention Kernels\n",
    "        self.kernels = {head_ind:{} for head_ind in range(n_heads)}\n",
    "        kernel_types = [\"query\", \"key\", \"value\"]\n",
    "        for head_ind in range(n_heads):\n",
    "            for kernel_type in kernel_types:\n",
    "                self.kernels[head_ind][kernel_type] = tf.keras.layers.Dense(self.d_model, \n",
    "                                                                            activation = 'relu')\n",
    "        self.attention_normalization = tf.keras.layers.Normalization()\n",
    "        self.linear_output_layer = tf.keras.layers.Dense(self.d_model, activation = 'relu')\n",
    "        \n",
    "        # Residual Connections\n",
    "        self.res_normalization_layer = tf.keras.layers.Normalization()\n",
    "        self.res_linear_layer = tf.keras.layers.Dense(self.d_model, \n",
    "                                                  activation = 'relu')  \n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "    def call(self, inputs, padding_mask = None):\n",
    "        '''\n",
    "        The attention block has three inputs:\n",
    "            - Queries\n",
    "            - Keys\n",
    "            - Values\n",
    "            \n",
    "        You can think of the keys as the \"memory\" of the attention block.\n",
    "        The attention block compares the queries to the keys to produce an attention matrix.\n",
    "        This attention matrix acts as a filter that is applied to the values, the output \n",
    "            of the model is a weighted average of the value sequence - containing only \n",
    "            information that the attention block deems important.\n",
    "        '''\n",
    "        \n",
    "        query_inp, key_inp, value_inp = inputs\n",
    "        # inputs - shape (batch_size, sentence_length, embedding_dim)\n",
    "        \n",
    "        ## Self-Attention Block\n",
    "        all_values = []\n",
    "        for head_ind in range(self.n_heads):         \n",
    "            # First we pass the queries and keys through \n",
    "            # their own dense layers. this allows each attention head\n",
    "            # to generate unique attention vectors\n",
    "            queries = self.kernels[head_ind][\"query\"](query_inp)\n",
    "            keys = self.kernels[head_ind][\"key\"](key_inp)\n",
    "            \n",
    "            # Compute the attention matrix\n",
    "            scores = tf.matmul(queries, tf.transpose(keys, perm = [0,2,1]))\n",
    "            \n",
    "            # Rescale the scores\n",
    "            d_k = (tf.shape(scores).numpy()[-1])\n",
    "            scaled_scores = (1/np.sqrt(d_k))*scores\n",
    "            \n",
    "            # (Optional) Apply a mask to the input\n",
    "            if padding_mask is not None:\n",
    "                scaled_scores += (padding_mask*-1e9)\n",
    "    \n",
    "            softmax_scores = tf.keras.activations.softmax(scaled_scores, axis = 2)\n",
    "        \n",
    "            # Apply the attention matrix to the values\n",
    "            values = tf.matmul(softmax_scores, tf.transpose(value_inp, perm = [0,1,2]))\n",
    "            all_values.append(values)\n",
    "        all_values = tf.concat(all_values, axis = 2)\n",
    "        normalized_values = self.attention_normalization(all_values)\n",
    "        linear_layer_values = self.dropout(self.linear_output_layer(normalized_values))\n",
    "        \n",
    "        # Residual Connection\n",
    "        # Add the attention block output to the original input and normalize\n",
    "        residual_connection = tf.math.add(query_inp, linear_layer_values)\n",
    "        linear_layer_out = self.res_linear_layer(residual_connection)\n",
    "        output = self.res_normalization_layer(linear_layer_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771d8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outerprod(input_x):\n",
    "    values = input_x[:, tf.newaxis, :]\n",
    "    values = tf.matmul(tf.transpose(values, perm = [0,2,1]), values)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f13150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq#[:, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_lookahead_mask(input_x):\n",
    "    # tf.linalg.band_part(input, -1, 0) set the lower diagonal to 0 \n",
    "    lookahead_mask = 1 - tf.linalg.band_part(tf.ones_like(input_x), -1, 0)\n",
    "    return tf.cast(lookahead_mask, tf.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d056db",
   "metadata": {},
   "source": [
    "Now we're ready to build the transformer model itself. The transformer outputs a probability vector of dimensions `(1,vocab_size)`, predicting the next token in the sequence. The new sequence is fed back into the model, and the process continues until we ask the model to stop.\n",
    "\n",
    "\n",
    "**Encoder Block**\n",
    "\n",
    "The initial sequence is tokenized and embedded using the positional encoding technique mentioned above. The embedded sequence is passed through a multi-headed attention layer. The output of the encoder block is a new sequence that represents a weighted average of the original sequence, containing only the information that the attention block deems important.\n",
    "![title](imgs/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db9173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, n_heads):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        super(Encoder, self).__init__(dynamic = True)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_length = input_shape[1]\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_dim = pt_vocab_size,\n",
    "                                                    output_dim = self.embedding_dim, \n",
    "                                                    input_length = input_length)\n",
    "        self.pos_encoding_layer = PositionalEncodingLayer(self.embedding_dim)\n",
    "        self.multiheadedattention = MultiHeadedAttention(n_heads = self.n_heads, d_model = self.embedding_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs - shape(batch_size, sentence_length , embedding_size)\n",
    "        padding_mask = create_padding_mask(outerprod(inputs))\n",
    "        \n",
    "        # Convert the tokenized sequence into embedded vectors\n",
    "        embedding = self.embedding_layer(inputs) \n",
    "        positional_embedding = self.pos_encoding_layer(embedding)\n",
    "        \n",
    "        # Self attention layer\n",
    "        attention = self.multiheadedattention([positional_embedding]*3, padding_mask)\n",
    "        return attention\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape((input_shape[1], self.embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e34ba",
   "metadata": {},
   "source": [
    "**Decoder Block**\n",
    "\n",
    "(1) The output from the previous timestep is tokenized and embedded using the positional encoding technique mentioned above. The embedded sequence is passed through a multi-headed attention layer. \n",
    "\n",
    "(2) We'll take the output from the encoder block and use it as the values and keys for another multi-headed attention block. the queries are the output of (1). The idea is to compare the transformer's output against the orignal sequence to produce the next output.\n",
    "\n",
    "(3) The output of (2) is passed through a softmax layer to produce a probability vector of size `(1, vocab_size)`, which is used to produce the next token.\n",
    "![title](imgs/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e69ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, n_heads):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        super(Decoder, self).__init__(dynamic = True)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_length = input_shape[1]\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_dim = pt_vocab_size,\n",
    "                                                    output_dim = self.embedding_dim, \n",
    "                                                    input_length = input_length)\n",
    "        self.pos_encoding_layer = PositionalEncodingLayer(self.embedding_dim)\n",
    "        self.multiheadedattention_1 = MultiHeadedAttention(n_heads = self.n_heads, \n",
    "                                                         d_model = self.embedding_dim)\n",
    "        self.multiheadedattention_2 = MultiHeadedAttention(n_heads = self.n_heads, \n",
    "                                                         d_model = self.embedding_dim)\n",
    "        self.softmax_layer = tf.keras.layers.Dense(en_vocab_size, activation = 'softmax')  \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs - shape(batch_size, sentence_length , embedding_size)\n",
    "        queries_encoder, keys_encoder, output_prev = inputs\n",
    "        \n",
    "        # Create padding masks\n",
    "        padding_mask = create_padding_mask(outerprod(output_prev))\n",
    "        lookahead_mask = create_lookahead_mask(outerprod(output_prev))\n",
    "        mask = tf.math.maximum(padding_mask, lookahead_mask)\n",
    "        \n",
    "        # Encode the output of the previous timestep\n",
    "        output_prev_embedded = self.pos_encoding_layer(self.embedding_layer(output_prev)) \n",
    "        # Pass the previous timestep's output through a self attention layer\n",
    "        attention_1 = self.multiheadedattention_1([output_prev_embedded]*3, mask)\n",
    "        \n",
    "        # Use the output of the previous timestep as the queries for a new attention layer\n",
    "        # The keys and values are the output of the encoder block\n",
    "        attention_2 = self.multiheadedattention_2([attention_1, queries_encoder, keys_encoder])\n",
    "        \n",
    "        # produce a probability vector to predict the next token in the sequence\n",
    "        probs = self.softmax_layer(attention_2)\n",
    "        return probs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return tf.TensorShape((input_shape[0][0], en_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f58dde",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840655e",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecfbd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder = tf.keras.layers.Input(shape = (None,))\n",
    "input_prev_output = tf.keras.layers.Input(shape = (None,))\n",
    "\n",
    "encoder_output = Encoder(d_model, n_heads)(input_encoder)\n",
    "p_tokens = Decoder(d_model, n_heads)([encoder_output, encoder_output, input_prev_output])#, transformer_output])\n",
    "\n",
    "model = tf.keras.Model(inputs = [input_encoder, input_prev_output], \n",
    "                       outputs = [p_tokens]) \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e180d",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c14c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [01:05<09:46, 65.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.88289, shape=(), dtype=float32)\n",
      "Portuguese: agora , os transgressores tambem beneficiam .\n",
      "Translation (Pred): ##rationsrations graphle claimduced grapholis tearsrations metabolic belong metabolicrations tearsolis belongduced claimle graphrations exit + discovery watched clicks graph discovery endingrationsrations belong theoryrations belongrations ending discovery graph clicks watched manage +rationsrations graph ending claim claim grapholis managerations metabolic belong graphrations discoveryolis belonguts claimle graphrations medicine metabolic belong watched clicks graph your clicksrationsrations neither opportunitiesrations belongrationsrations discovery graph clicks watched manage +rationsrations readerrations + claim grapholis metabolicrations metabolic\n",
      "Translation (True): now , the offenders , they also benefit .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [02:09<08:36, 64.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.878183, shape=(), dtype=float32)\n",
      "Portuguese: agora , os transgressores tambem beneficiam .\n",
      "Translation (Pred): ##rationsrations graphle claimduced grapholis belongrations metabolic belong metabolicrations belongolis belong iron claimle graphrations exit + discovery watched clicks graph your endingrationsrationsrations opportunitiesrations belongrations ending discovery graph clicks watched manage +rationsrations graph ending claim claim grapholis managerations metabolic belong watchedrations discoveryolis belonguts claimle graphrations medicine metabolic belong watched clicks graph your worstrationsrations neither opportunitiesrations belongrationsrations discovery graph clicks watched manage +rationsrations graphrations + claim grapholis metabolicrations metabolic\n",
      "Translation (True): now , the offenders , they also benefit .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [03:12<07:27, 63.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.873693, shape=(), dtype=float32)\n",
      "Portuguese: agora , os transgressores tambem beneficiam .\n",
      "Translation (Pred): ##rationsrations graphle claimduced grapholis belongrations metabolic belong metabolicrations belongolis belong iron claimle graphrations exit + discovery medicine clicks graph your endingrationsrationsrations opportunitiesrations belongrations ending discovery graph clicks watched manage +rationsrations graph ending + claim grapholis watchedrations metabolic belong watchedrations discoveryolis belong iron claimle graphrations medicine metabolic belong watched clicks graph your worstrationsrations neither opportunitiesrationstrationsrations discovery graph clicks watched manage +rationsrations graphrations + claim grapholis metabolicrations metabolic\n",
      "Translation (True): now , the offenders , they also benefit .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [04:15<06:21, 63.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.869382, shape=(), dtype=float32)\n",
      "Portuguese: agora , os transgressores tambem beneficiam .\n",
      "Translation (Pred): ##rationsrations graph astduced grapholis belongrations metabolic belong metabolicrations belongolis belong iron claimle graphrations exit + discovery medicine clicks graph your endingrationsrationsrations opportunitiesrationstrations ending your graph clicks watched manage +rationsrations reader ast claim grapholis watchedrations metabolic belong medicinerations worldolis belong ironlele graphrations medicine metabolic metabolic medicine clicks graph your endingrationsrations neither opportunitiesrationstrationsrations discovery graph clicks watched manage +rations metabolic graphrations + claim graphle metabolicrations metabolic\n",
      "Translation (True): now , the offenders , they also benefit .\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "for ep in tqdm(range(n_episodes)):\n",
    "    \n",
    "    # Iterate through each batch\n",
    "    for batch_ind, (port, eng) in enumerate(train_dataset_batched):  \n",
    "        prev_transformer_output = tf.zeros((batch_size, tf.shape(eng)[1].numpy()),dtype = tf.int64)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Query Model\n",
    "            token_probs = model([port,prev_transformer_output])\n",
    "            next_token = tf.argmax(token_probs, axis = 2)\n",
    "            \n",
    "            # Calculate loss\n",
    "            eng_one_hot = tf.one_hot(eng, depth = en_vocab_size)\n",
    "            cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "            loss = cce(eng_one_hot, token_probs)\n",
    "        \n",
    "        # Update Model Parameters\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        #if batch_ind == 300: break\n",
    "        \n",
    "    # Compare the predicted translation to the true translation\n",
    "    translation_pred = tokenizers.en.detokenize(next_token)\n",
    "    translation_true = tokenizers.en.detokenize(eng)\n",
    "    orig = tokenizers.pt.detokenize(port)\n",
    "        \n",
    "    for pt_line,en_line_pred, en_line_true in zip(orig.numpy(), translation_pred.numpy(), translation_true.numpy()):\n",
    "        print(loss)\n",
    "        print(f\"Portuguese: {pt_line.decode('utf-8')}\")\n",
    "        print(f\"Translation (Pred): {en_line_pred.decode('utf-8')}\")\n",
    "        print(f\"Translation (True): {en_line_true.decode('utf-8')}\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
